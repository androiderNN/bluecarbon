{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m./raws/train_data.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m./raws/test_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mMIN_WVP\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m test \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mMIN_WVP\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/08-bluecarbon-CvTXT7p5/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "train = pd.read_csv('./raws/train_data.csv')\n",
    "test = pd.read_csv('./raws/test_data.csv')\n",
    "\n",
    "train = train.loc[:, 'lat':'MIN_WVP']\n",
    "test = test.loc[:, 'lat':'MIN_WVP']\n",
    "\n",
    "train = train.drop(columns=['Landsat_StartTime', 'PRODUCT_ID'])\n",
    "test = test.drop(columns=['Landsat_StartTime', 'PRODUCT_ID'])\n",
    "\n",
    "pickle.dump(train, open('./processed/shrinked_train.pkl', 'wb'))\n",
    "pickle.dump(test, open('./processed/shrinked_test.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下本体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurecreation(df, istrain:bool):\n",
    "    # 切り出し\n",
    "    # df = df.loc[:, 'lat':'sst_diff']\n",
    "    df = df.loc[:, 'lat':'MIN_WVP']\n",
    "\n",
    "    # 周辺調査地のデータ挿入\n",
    "    a = list(df.loc[:, 'lat'])\n",
    "    o = list(df.loc[:, 'lon'])\n",
    "    group = [str(round(a[i], 2))+'_'+str(round(o[i], 2)) for i in range(len(a))]\n",
    "    # group = [str(round(a[i], 1))+'_'+str(round(o[i], 1)) for i in range(len(a))]\n",
    "    if istrain == True:\n",
    "        global c, temp\n",
    "        c = list(df.loc[:, 'cover'])\n",
    "        temp = {i: [[a[j], o[j], c[j]] for j in range(len(c)) if group[j]==i]\n",
    "                for i in np.unique(group)}\n",
    "    \n",
    "    # dist = [{(a[j]-i[0])**2 + (o[j]-i[1])**2 : i[2] for i in temp[group[j]]}\n",
    "    #         for j in range(len(a))]\n",
    "    # dist = [sorted(i.items()) + [(100, np.nan)]*3 for i in dist]\n",
    "\n",
    "    dist = []\n",
    "    for i in range(len(a)):\n",
    "        td = {}\n",
    "        try:\n",
    "            for j in temp[group[i]]:\n",
    "                td[(a[i]-j[0])**2 + (o[i]-j[1])**2] = j[2]\n",
    "        except:\n",
    "            td = {1: np.nan, 2: np.nan, 3: np.nan}\n",
    "        dist.append(td)\n",
    "\n",
    "    dist = [sorted(i.items()) for i in dist]\n",
    "    n1 = []\n",
    "    n3 = []\n",
    "    for i in dist:\n",
    "        # cover_nearest\n",
    "        try:\n",
    "            if istrain == True:\n",
    "                n1.append(i[1][1])\n",
    "            else:\n",
    "                n1.append(i[0][1])\n",
    "        except (KeyError, IndexError):\n",
    "            n1.append(np.nan)\n",
    "        # cover_3\n",
    "        try:\n",
    "            if istrain == True:\n",
    "                n3.append(mean([i[1][1], i[2][1], i[3][1]]))\n",
    "            else:\n",
    "                n3.append(mean([i[0][1], i[1][1], i[2][1]]))\n",
    "        except (KeyError, IndexError):\n",
    "            n3.append(np.nan)\n",
    "\n",
    "    df['cover_nearest'] = n1\n",
    "    df['cover_3'] = n3\n",
    "    \n",
    "    # dd_df = df.loc[:, ['lon', 'lat']].values\n",
    "    # if istrain == True:\n",
    "    #     nbrs = NearestNeighbors(n_neighbors=3).fir(dd_df)\n",
    "    \n",
    "    # # 海岸の比率\n",
    "    # df['beach_rate'] = df['beach_length'] / df['coast_length']\n",
    "    # df['aicial_rate'] = df['aicial_length'] / df['coast_length']\n",
    "\n",
    "    # 透明度推定\n",
    "    # df['SS_est'] = (df['Green']-df['SWIR2']) / (df['Red']-df['SWIR2'])\n",
    "    df['Chla_est'] = (df['Blue']-df['SWIR2']) / (df['Red']-df['SWIR2'])\n",
    "\n",
    "    # その他\n",
    "    df['slope'] = df['depth_original'] / df['coastal_dist']\n",
    "    # df['sdiff'] = df['warm_sst'] - df['cold_sst']\n",
    "\n",
    "    # depthの処理\n",
    "    # df.loc[df['depth']==0, 'depth'] = np.nan\n",
    "    # df['dfdiff'] = df['depth_original'] - df['depth']\n",
    "    # df.loc[~(df['depth_original'].isna()), 'depth'] = np.nan\n",
    "\n",
    "    # 文献の分類\n",
    "    # df['YMDstyle'] = [str(str(i).count('-')) +'_'+ str(str(i).count('.')) for i in list(df['YMD'])]\n",
    "    # df['YMDstyle'].replace(['0_4', '1_2', '2_12', '0_2'], np.nan, inplace=True)\n",
    "    # df['YMDstyle'].replace(['0_0', '0_1', '0_2', '0_4', '1_2', '1_4', '2_12'], [2, 1, 3, np.nan, np.nan, 4, np.nan], inplace=True)\n",
    "\n",
    "    # 必要な列の抽出\n",
    "    col = ['Chla_est', 'cover_nearest', 'slope', 'cover_3']\n",
    "    # col = ['Chla_est', 'cover_3']\n",
    "    df = pd.concat([df.loc[:, 'lat':'sst_diff'], df.loc[:, col]], axis=1)\n",
    "    col = ['year', 'month', 'YMD', 'area', 'lon', 'lat']\n",
    "    col = col + ['cliff_length', 'Salinity_annual', 'river_area', 'sst_diff']\n",
    "    col = col + ['cold_sst', 'coast_length', 'sst_ymd', 'sst_annual']\n",
    "    # col = col + ['river_dist', 'aicial_length', 'beach_length', 'coastal_dist', 'depth']\n",
    "    df.drop(columns=col, inplace=True)\n",
    "    # df = df.loc[:, ['cover', 'depth_original','area','YMDstyle','warm_sst','fetch','depth','hist_warm_sst','coastal_dist','Salinity_annual','coast_length','aicial_length','sst_diff']]\n",
    "\n",
    "    # スタッキング用にバリデーションのFoldを追加\n",
    "    df_ = pd.DataFrame()\n",
    "    if istrain == True:\n",
    "        l = [i for i in range(4) for j in range(math.floor(len(df)/4))] + [3]*(len(df)-(round(len(df)/4)*4))\n",
    "        df['valfold'] = random.sample(l, len(l))\n",
    "        df_ = df\n",
    "        # for i in range(4):\n",
    "        #     df_ = pd.concat([df_, df.replace({'valfold': [(i+j+1)%4 for j in range(3)]}, i*11)])\n",
    "    elif istrain == False:\n",
    "        df_ = df    \n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_estimation(df, estlist):\n",
    "    model_dict = {}\n",
    "\n",
    "    df = df.iloc[:14139, :]\n",
    "\n",
    "    for col in estlist:\n",
    "        print('\\n===========================================================\\n'\n",
    "        'estimating ' + col + '\\n')\n",
    "\n",
    "        train_df = df.loc[~(df[col].isna()), ]\n",
    "\n",
    "        x = train_df.drop(columns=['cover', 'valfold', col], errors='ignore')\n",
    "        y = train_df[col]\n",
    "\n",
    "        kf = KFold(n_splits=4, shuffle=True, random_state=234)\n",
    "        c = 0\n",
    "        model = []\n",
    "        \n",
    "        for idx_tr, idx_va in kf.split(train_df):\n",
    "            tr_x, va_x = x.iloc[idx_tr], x.iloc[idx_va]\n",
    "            tr_y, va_y = y.iloc[idx_tr], y.iloc[idx_va]\n",
    "\n",
    "            lgb_train = lgb.Dataset(tr_x, tr_y)\n",
    "            lgb_val = lgb.Dataset(va_x, va_y)\n",
    "\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'verbose': -1,\n",
    "                'random_state': 254,\n",
    "                # 'min_data_in_leaf': 200,\n",
    "                # 'bagging_fraction': 0.5,\n",
    "                # 'bagging_freq': 1,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5,\n",
    "            }\n",
    "\n",
    "            model.append(lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                valid_sets=[lgb_train, lgb_val],\n",
    "                valid_names=['train', 'val'],\n",
    "                num_boost_round=1000,\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=2, verbose=True)]\n",
    "            ))\n",
    "        model_dict[col] = model\n",
    "\n",
    "    print('\\n===========================================================\\n'\n",
    "    'finished estimating.')\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "def predict_na(df, model):\n",
    "    df_ = df\n",
    "\n",
    "    for col in model.keys():\n",
    "        for i in range(4):\n",
    "            df_[col + 'pred' + str(i)] = model[col][i].predict(df, predict_disable_shape_check=True)\n",
    "        \n",
    "        df_[col + '_pred'] = df_[[col+'pred'+str(i) for i in range(4)]].mean(axis=1)\n",
    "        df_[col].fillna(df_[col+'_pred'], inplace=True)\n",
    "        df_.drop(columns=[col+'pred'+str(i) for i in range(4)]+[col+'_pred'], inplace=True)\n",
    "\n",
    "    return df_\n",
    "\n",
    "def make_nn_data(df, istrain:bool):\n",
    "    # NA推定\n",
    "    estlist = ['depth_original', 'Chla_est']\n",
    "    if istrain == True:\n",
    "        global fillmodel\n",
    "        fillmodel = na_estimation(df, estlist)\n",
    "    df = predict_na(df, fillmodel)\n",
    "\n",
    "    # 穴埋め\n",
    "    for col in df.columns:\n",
    "        df[col].fillna(df[col].mean(numeric_only=True), inplace=True)\n",
    "    \n",
    "    #標準化\n",
    "    if istrain == True:\n",
    "        global mea\n",
    "        global std\n",
    "        mea = df.drop(columns=['cover', 'valfold']).mean()\n",
    "        std = df.drop(columns=['cover', 'valfold']).std()\n",
    "        cover = df['cover']\n",
    "        valfold = df['valfold']\n",
    "        df = (df - mea) / std\n",
    "        df['cover'] = cover\n",
    "        df['valfold'] = valfold\n",
    "    else:\n",
    "        df = (df - mea) / std\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "# train = pd.read_csv('./raws/train_data.csv')\n",
    "# test = pd.read_csv('./raws/test_data.csv')\n",
    "train = pickle.load(open('./processed/shrinked_train.pkl', 'rb'))\n",
    "test = pickle.load(open('./processed/shrinked_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================\n",
      "estimating depth_original\n",
      "\n",
      "Training until validation scores don't improve for 2 rounds\n",
      "Early stopping, best iteration is:\n",
      "[211]\ttrain's rmse: 0.262824\tval's rmse: 0.334566\n",
      "Training until validation scores don't improve for 2 rounds\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttrain's rmse: 0.322443\tval's rmse: 0.616311\n",
      "Training until validation scores don't improve for 2 rounds\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttrain's rmse: 0.293195\tval's rmse: 0.463553\n",
      "Training until validation scores don't improve for 2 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\ttrain's rmse: 0.354123\tval's rmse: 0.402986\n",
      "\n",
      "===========================================================\n",
      "estimating Chla_est\n",
      "\n",
      "Training until validation scores don't improve for 2 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttrain's rmse: 0.230031\tval's rmse: 0.319952\n",
      "Training until validation scores don't improve for 2 rounds\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttrain's rmse: 0.204161\tval's rmse: 0.290772\n",
      "Training until validation scores don't improve for 2 rounds\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttrain's rmse: 0.193448\tval's rmse: 0.291788\n",
      "Training until validation scores don't improve for 2 rounds\n",
      "Early stopping, best iteration is:\n",
      "[114]\ttrain's rmse: 0.189917\tval's rmse: 0.304459\n",
      "\n",
      "===========================================================\n",
      "finished estimating.\n"
     ]
    }
   ],
   "source": [
    "train_lgb = featurecreation(train, True)\n",
    "test_lgb = featurecreation(test, False)\n",
    "\n",
    "# df = train_lgb[train_lgb['cover']>0.5]\n",
    "# for i in range(5):\n",
    "#     train_lgb = pd.concat([train_lgb, df])\n",
    "# train_lgb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pickle.dump(train_lgb, open('./processed/nan_train.pkl', 'wb'))\n",
    "pickle.dump(test_lgb, open('./processed/nan_test.pkl', 'wb'))\n",
    "\n",
    "train_nn = make_nn_data(train_lgb, True)\n",
    "test_nn = make_nn_data(test_lgb, False)\n",
    "\n",
    "pickle.dump(train_nn, open('./processed/filled_train.pkl', 'wb'))\n",
    "pickle.dump(test_nn, open('./processed/filled_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cover</th>\n",
       "      <th>depth_original</th>\n",
       "      <th>aicial_length</th>\n",
       "      <th>beach_length</th>\n",
       "      <th>coastal_dist</th>\n",
       "      <th>depth</th>\n",
       "      <th>fetch</th>\n",
       "      <th>hist_cold_sst</th>\n",
       "      <th>hist_warm_sst</th>\n",
       "      <th>river_dist</th>\n",
       "      <th>warm_sst</th>\n",
       "      <th>Chla_est</th>\n",
       "      <th>cover_nearest</th>\n",
       "      <th>slope</th>\n",
       "      <th>cover_3</th>\n",
       "      <th>valfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050</td>\n",
       "      <td>6.218322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5590.0</td>\n",
       "      <td>23.664240</td>\n",
       "      <td>2590.515869</td>\n",
       "      <td>20.798887</td>\n",
       "      <td>31.061762</td>\n",
       "      <td>126.328598</td>\n",
       "      <td>29.149746</td>\n",
       "      <td>3.648817</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.075</td>\n",
       "      <td>7.242031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>877.570068</td>\n",
       "      <td>200.0</td>\n",
       "      <td>24.079815</td>\n",
       "      <td>1153.718506</td>\n",
       "      <td>17.963797</td>\n",
       "      <td>31.112072</td>\n",
       "      <td>7.336063</td>\n",
       "      <td>29.178904</td>\n",
       "      <td>3.353233</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.150</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1718.315796</td>\n",
       "      <td>20.647753</td>\n",
       "      <td>31.402370</td>\n",
       "      <td>10.295314</td>\n",
       "      <td>29.357115</td>\n",
       "      <td>3.926495</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.150</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.050</td>\n",
       "      <td>8.105741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1329.338867</td>\n",
       "      <td>117.5</td>\n",
       "      <td>5.412169</td>\n",
       "      <td>1144.260986</td>\n",
       "      <td>20.089460</td>\n",
       "      <td>31.146654</td>\n",
       "      <td>37.656109</td>\n",
       "      <td>28.946106</td>\n",
       "      <td>4.019800</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>579.048828</td>\n",
       "      <td>338.0</td>\n",
       "      <td>6.978470</td>\n",
       "      <td>508.141266</td>\n",
       "      <td>21.131441</td>\n",
       "      <td>31.394224</td>\n",
       "      <td>3.806094</td>\n",
       "      <td>29.387054</td>\n",
       "      <td>2.949698</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14124</th>\n",
       "      <td>0.025</td>\n",
       "      <td>8.554358</td>\n",
       "      <td>14.368234</td>\n",
       "      <td>855.136963</td>\n",
       "      <td>325.0</td>\n",
       "      <td>9.134441</td>\n",
       "      <td>661.605347</td>\n",
       "      <td>17.726496</td>\n",
       "      <td>31.248995</td>\n",
       "      <td>5.513156</td>\n",
       "      <td>29.086094</td>\n",
       "      <td>3.085735</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14130</th>\n",
       "      <td>0.050</td>\n",
       "      <td>6.218322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8300.0</td>\n",
       "      <td>21.561544</td>\n",
       "      <td>2617.064453</td>\n",
       "      <td>20.805063</td>\n",
       "      <td>31.091637</td>\n",
       "      <td>127.528763</td>\n",
       "      <td>29.172035</td>\n",
       "      <td>3.928402</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.050</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14131</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1175.292236</td>\n",
       "      <td>330.0</td>\n",
       "      <td>0.879137</td>\n",
       "      <td>1127.085205</td>\n",
       "      <td>21.265642</td>\n",
       "      <td>31.285402</td>\n",
       "      <td>15.682171</td>\n",
       "      <td>29.331314</td>\n",
       "      <td>3.540347</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14135</th>\n",
       "      <td>0.050</td>\n",
       "      <td>8.105741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.749870</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.322502</td>\n",
       "      <td>1372.131348</td>\n",
       "      <td>20.803297</td>\n",
       "      <td>31.215786</td>\n",
       "      <td>128.361176</td>\n",
       "      <td>29.155453</td>\n",
       "      <td>3.385380</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14137</th>\n",
       "      <td>0.050</td>\n",
       "      <td>3.567922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>594.742188</td>\n",
       "      <td>530.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1158.051880</td>\n",
       "      <td>20.867407</td>\n",
       "      <td>31.119274</td>\n",
       "      <td>108.605507</td>\n",
       "      <td>29.197662</td>\n",
       "      <td>2.689846</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5873 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cover  depth_original  aicial_length  beach_length  coastal_dist  \\\n",
       "0      0.050        6.218322       0.000000      0.000000        5590.0   \n",
       "6      0.075        7.242031       0.000000    877.570068         200.0   \n",
       "7      0.150        2.000000       0.000000      0.000000        4100.0   \n",
       "9      0.050        8.105741       0.000000   1329.338867         117.5   \n",
       "11     0.000        0.400000       0.000000    579.048828         338.0   \n",
       "...      ...             ...            ...           ...           ...   \n",
       "14124  0.025        8.554358      14.368234    855.136963         325.0   \n",
       "14130  0.050        6.218322       0.000000      0.000000        8300.0   \n",
       "14131  0.375        0.900000       0.000000   1175.292236         330.0   \n",
       "14135  0.050        8.105741       0.000000     56.749870          10.0   \n",
       "14137  0.050        3.567922       0.000000    594.742188         530.0   \n",
       "\n",
       "           depth        fetch  hist_cold_sst  hist_warm_sst  river_dist  \\\n",
       "0      23.664240  2590.515869      20.798887      31.061762  126.328598   \n",
       "6      24.079815  1153.718506      17.963797      31.112072    7.336063   \n",
       "7       1.000000  1718.315796      20.647753      31.402370   10.295314   \n",
       "9       5.412169  1144.260986      20.089460      31.146654   37.656109   \n",
       "11      6.978470   508.141266      21.131441      31.394224    3.806094   \n",
       "...          ...          ...            ...            ...         ...   \n",
       "14124   9.134441   661.605347      17.726496      31.248995    5.513156   \n",
       "14130  21.561544  2617.064453      20.805063      31.091637  127.528763   \n",
       "14131   0.879137  1127.085205      21.265642      31.285402   15.682171   \n",
       "14135   0.322502  1372.131348      20.803297      31.215786  128.361176   \n",
       "14137   5.000000  1158.051880      20.867407      31.119274  108.605507   \n",
       "\n",
       "        warm_sst  Chla_est  cover_nearest     slope  cover_3  valfold  \n",
       "0      29.149746  3.648817          0.050      -inf    0.050        2  \n",
       "6      29.178904  3.353233          0.075      -inf    0.075        0  \n",
       "7      29.357115  3.926495          0.150  0.000488    0.150        2  \n",
       "9      28.946106  4.019800          0.050      -inf    0.050        1  \n",
       "11     29.387054  2.949698          0.000  0.001183    0.000        2  \n",
       "...          ...       ...            ...       ...      ...      ...  \n",
       "14124  29.086094  3.085735          0.025      -inf    0.025        0  \n",
       "14130  29.172035  3.928402          0.050      -inf    0.050        3  \n",
       "14131  29.331314  3.540347          0.375  0.002727    0.375        0  \n",
       "14135  29.155453  3.385380          0.050      -inf    0.050        1  \n",
       "14137  29.197662  2.689846          0.050      -inf    0.050        0  \n",
       "\n",
       "[5873 rows x 16 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "08-bluecarbon-CvTXT7p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "276441da38dc6a3e251be116a0d14b55ec747c854d559d41bae05c76a1593c62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
